# Pipeline

## Objective

Build an experimental data pipeline which aims at automating the process of data science and relieving the burden of repetitive work. 

## Several Concrete Goals

__Note:__ the title will be replaced by __Menu__ or __Content__ in the future.


1. Data Cleaning
2. Data Loading
3. Exploray Data Analysis
4. Data Visualization
5. Business/ Domain Field
6. ETL
7. Feature Engineering
8. Feature Extraction (Automatic)
9. Reuseable Features
10. Feature Selection (Automatic)
11. Automatic Validation and Evaluation of Features
   + Performance
   + Entropy
12. Model Training
13. Model Selection & Validation
14. Easily Deployment on the Production Environment
15. Report Generation

## Data Cleaning

1. Data Type
   + Text
   + Geological
2. Missing Values

## ETL

1. GPU
2. Parallel Computing
3. Multithreading
4. Distributed Computing
5. Real-time Processing

## Feature Selection

1. Covariance Matrix
2. [3 Effective Feature Selection Strategies](https://medium.com/towards-data-science/three-effective-feature-selection-strategies-e1f86f331fb1)
3. PCA
4. ICA
5. Decision Tree
6. Linear and Quadratic Discriminant Analysis

## Model

1. AutoML

## Reference

1. [Using Machine Learning to Predict Value of Homes On Airbnb](https://medium.com/airbnb-engineering/using-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d)
2. [AutoML](http://www.ml4aad.org/automl/)
3. mrjob
4. Flink
5. Hadoop/ Spark
6. [Spark vs Flink](https://stackoverflow.com/questions/28082581/what-is-the-difference-between-apache-spark-and-apache-flink)